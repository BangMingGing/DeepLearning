{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 파이썬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 파이썬에서는 tab보다 공백 문자 4개를 권장한다. (검색해보니 \"파이썬 코딩 스타일 가이드(PEP 8)에서는 공백 4칸으로 규정하고 있습니다\" 라고 한다)\n",
    "2. class의 메소드들은 첫 번째 인수로 자신(self)를 사용한다.\n",
    "3. 브로드 캐스트 : 형상이 다른 배열끼리 연산\n",
    "4. 파이썬(동적 언어)는 C,C++(정적 언어)보다 처리속도가 느리다. -> 빠른 성능이 요구되는 부분은 C,C++로 해결 (Numpy도 주된 처리는 C,  C++로 구현)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0, 6, 0.1)\n",
    "y1 = np.sin(x)\n",
    "y2 = np.cos(x)\n",
    "\n",
    "plt.plot(x, y1, label =\"sin\")\n",
    "plt.plot(x, y2, linestyle =\"--\", label =\"cos\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"sin&cos\")\n",
    "plt.legend() # 8, 9번 줄의 label을 자동으로 가져온다.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 퍼셉트론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND(x1, x2):\n",
    "    w1, w2, theta = 0.5, 0.5, 0.7 # AND 진리표를 만족하는 임의의 값을 넣어줌\n",
    "    tmp = x1*w1 + x2*w2\n",
    "    if tmp <= theta:\n",
    "        return 0\n",
    "    elif tmp > theta:\n",
    "        return 1\n",
    "\n",
    "print(AND(0, 0))\n",
    "print(AND(0, 1))\n",
    "print(AND(1, 0))\n",
    "print(AND(1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "편향을 도입\n",
    "b = -theta 로 해서\n",
    "b + w1x1 + w2x2 가 0보다 작으면 0, 크면 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([0, 1])\n",
    "w = np.array([0.5, 0.5])\n",
    "b = -0.7\n",
    "print(w*x)\n",
    "print(np.sum(w*x))\n",
    "print(np.sum(w*x) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([0.5, 0.5])\n",
    "    b = -0.7\n",
    "    tmp = np.sum(w*x) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "print(AND(0, 0))\n",
    "print(AND(0, 1))\n",
    "print(AND(1, 0))\n",
    "print(AND(1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치와 편향은 기능이 다르다.\n",
    "\n",
    "가중치는 입력신호가 결과에 주는 영향력(중요도) 조절하는 매개변수\n",
    "편향은 뉴런이 얼마나 쉽게 활성화하느냐를 조정하는 매개변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAND 게이트\n",
    "\n",
    "def NAND(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([-0.5, -0.5])\n",
    "    b = 0.7\n",
    "    tmp = np.sum(w*x) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "print(NAND(0, 0))\n",
    "print(NAND(0, 1))\n",
    "print(NAND(1, 0))\n",
    "print(NAND(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR 게이트 (AND게이트와 가중치 w, b만 다르다)\n",
    "# b가 두 개의 w보다 작아야한다? - 내 생각\n",
    "\n",
    "def OR(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([0.5, 0.5])\n",
    "    b = -0.2\n",
    "    tmp = np.sum(w*x) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "print(OR(0, 0))\n",
    "print(OR(0, 1))\n",
    "print(OR(1, 0))\n",
    "print(OR(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR게이트는 배타적 논리합이라는 논리 회로.\n",
    "둘 중 하나만 1일때 1을 출력\n",
    "\n",
    "퍼셉트론으로는 비선형 영역을 나타낼 수 없어서 XOR 게이트를 표현 불가\n",
    "-> \"다층 퍼셉트론을 만들 수 있다\"는 점에서 장점이 있다.\n",
    "\n",
    "조합을 통해서 XOR게이트를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR(x1, x2):\n",
    "    s1 = NAND(x1, x2)\n",
    "    s2 = OR(x1, x2)\n",
    "    y = AND(s1, s2)\n",
    "    return y\n",
    "\n",
    "print(XOR(0, 0))\n",
    "print(XOR(0, 1))\n",
    "print(XOR(1, 0))\n",
    "print(XOR(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR게이트는 다층 퍼셉트론이다\n",
    "\n",
    "퍼셉트론은 층을 거듭 쌓으면 비선형적인 표현도 가능하고 컴퓨터가 수행하는 처리 모두 표현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 신경망\n",
    "\n",
    "퍼셉트론의 가중치들을 사람이 수동적으로 해야한다 -> 신경망이 이것을 해결.\n",
    "\n",
    "가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력이 신경망의 중요한 성질이다.\n",
    "\n",
    "신경먕은 입력층 -> 은닉층 -> 출력층 으로 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "활성화 함수 : 활성화하는 역할 ( 0보다 크면 1 아니면 0 출력 - 계단 함수 )\n",
    "ex) y = h(b + w1x1 + w2x2)\n",
    "\n",
    "활성화 함수는 퍼셉트론에서 신경망으로 가기 위한 길잡이\n",
    "\n",
    "*일반적으로 \n",
    "(단순) 퍼셉트론 : 단층 네트워크에서 계단 함수를 활성화 함수로 사용한 모델\n",
    "다층 퍼셉트론 : 신경망 (시그모이드 함수 등의 매끈한 활성화 함수를 사용하는 네트워크)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "활성화 함수 (계단 함수를 다른 함수로 변경하는 것이 신경망으로 나아가는 열쇠)\n",
    "\n",
    "(0). 계단 함수\n",
    "h(x) = 1 if x>0 / 0 else\n",
    "\n",
    "1. 시그모이드 함수\n",
    "h(x) = 1 / 1 + e^-x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계단 함수 구현\n",
    "\n",
    "def step_function(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def step_function(x): # 넘파이 배열까지 지원하는 계단 함수\n",
    "    y = x > 0\n",
    "    return y.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계단 함수 그래프\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def step_function(x):\n",
    "    return np.array(x > 0, dtype = np.int)\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1) # x 값 설정\n",
    "y = step_function(x) # y값 설정\n",
    "plt.plot(x, y) # 그리기\n",
    "plt.ylim(-0.1, 1.1) # y값 그래프 범위 설정\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시그모이드 함수 구현\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.array([-1.0, 1.0, 2.0])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시그모이드 함수 그래프\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계단 함수와 비교했을때 시그모이드 함수는 곡선을 나타낸다.\n",
    "-> 이 점이 신경망 학습에서 아주 중요한 역할을 한다.\n",
    "\n",
    "공통점 : 비선형 함수이다. (하나의 직선으로 그릴 수 없다)\n",
    "-> 신경망에서는 활성화 함수로 비선형 함수를 사용해야함.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU 함수\n",
    "h(x) = x if x>0 / 0 else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReLU 함수 그래프\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = relu(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.5, 5.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행렬의 곱 - 1차원:벡터, 2차원:행렬 곱\n",
    "np.dot(A, B) != np.dot(B, A)\n",
    "\n",
    "2x3행렬, 2x2 행렬 곱 연산시 오류 출력 A의 열 수와 B의 행 수를 맞추어야함\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력층에서 1층으로 신호 전달\n",
    "X = np.array([1.0, 0.5])\n",
    "W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "B1 = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "print(W1.shape)\n",
    "print(X.shape)\n",
    "print(B1.shape)\n",
    "\n",
    "# 전달된 신호\n",
    "A1 = np.dot(X, W1) + B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1층에서 활성화 함수 사용 후 Z1에 저장\n",
    "Z1 = sigmoid(A1)\n",
    "\n",
    "print(A1)\n",
    "print(Z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1층에서 2층으로 신호 전달\n",
    "\n",
    "W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "B2 = np.array([0.1, 0.2])\n",
    "\n",
    "print(Z1.shape)\n",
    "print(W2.shape)\n",
    "print(B2.shape)\n",
    "\n",
    "# 전달된 신호\n",
    "A2 = np.dot(Z1, W2) + B2\n",
    "\n",
    "# 2층에서 활성화 함수 사용\n",
    "Z2 = sigmoid(A2)\n",
    "\n",
    "print(A2)\n",
    "print(Z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2층에서 출력층으로 신호 전달\n",
    "\n",
    "def identity_function(x): # 항등 함수 (흐름을 통일하기 위해 그냥 구현)\n",
    "    return x\n",
    "\n",
    "W3 = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "B3 = np.array([0.1, 0.2])\n",
    "\n",
    "A3 = np.dot(Z2, W3) + B3\n",
    "Y = identity_function(A3)\n",
    "\n",
    "print(A3)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구현 정리\n",
    "\n",
    "def init_network(): # 가중치와 편향 초기화\n",
    "    network = {}\n",
    "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network['B1'] = np.array([0.1, 0.2, 0.3])\n",
    "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network['B2'] = np.array([0.1, 0.2])\n",
    "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "    network['B3'] = np.array([0.1, 0.2])\n",
    "\n",
    "    return network\n",
    "\n",
    "def forward(network, x): # 신호가 순방향으로 전달\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    B1, B2, B3 = network['B1'], network['B2'], network['B3']\n",
    "\n",
    "    a1 = np.dot(x, W1) + B1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + B2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + B3\n",
    "    y = identity_function(a3)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "network = init_network()\n",
    "x = np.array([1.0, 0.5])\n",
    "y = forward(network, x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 소프트맥스 함수\n",
    "\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 출력층의 뉴런 수 정하기\n",
    "\n",
    "분류에서 분류하고 싶은 클래스 수로 설정하는 것이 일반적임\n",
    "ex) 숫자 0~9 분류 -> 출력층 뉴런은 10개\n",
    "\n",
    "## 손글씨 숫자 인식\n",
    "\n",
    "학습된 매개변수 사용, 학습 과정 생략, 추론 과정(순전파)만 구현\n",
    "\n",
    "### MNIST 데이터셋\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(flatten=True, normalize=False)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST의 인수 3 가지 (bool 값)\n",
    "1. normalize : True 픽셀 값을 0~1.0, False 픽셀 값을 0부터 255 사용\n",
    "2. flatten : True 이미지를 1차원 배열로, False 3차원 배열로\n",
    "3. one-hot-encoding : True 0, 1의 배열로 나타냄, False 숫자 형태의 레이블 저장 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망의 추론 처리\n",
    "\n",
    "입력층 뉴런 : 784개 (28*28, 아마 픽셀의 개수?)\n",
    "출력층 뉴런 : 10개 (0~9 숫자 분류)\n",
    "은닉층 뉴런 : 1번-50개 뉴런, 2번-100개 뉴런 (임의의 개수)\n",
    "\n",
    "\n",
    "정규화(normalization) : 데이터를 특정 범위로 변환하는 처리 (전처리의 일종?)\n",
    "전처리(pre-processing) : 신경망의 입력 데이터에 특정 변환을 가하는 것\n",
    "\n",
    "데이터 백색화(whitening) : 데이터를 균일하게 분포시키는 정규화?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = \\\n",
    "        load_mnist(normalize=False, flatten=True, one_hot_label=False)\n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    with open(\"./ch03/sample_weight.pkl\", 'rb') as f: # .이 현재 디렉토리를 뜻함\n",
    "        network = pickle.load(f)\n",
    "\n",
    "    return network\n",
    "\n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "accuracy_cnt = 0\n",
    "\n",
    "for i in range(len(x)):\n",
    "    y = predict(network, x[i])\n",
    "    p = np.argmax(y)\n",
    "    if p == t[i]:\n",
    "        accuracy_cnt += 1\n",
    "    \n",
    "print(\"Accuray:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 배치 처리\n",
    "\n",
    "전체적으로 보면 28*28인 2차원 배열 -> 원소가 10개인 1차원 배열 출력\n",
    "\n",
    "배치(batch) : 하나로 묶은 입력 데이터\n",
    "(수치 계산 라이브러리는 대부분 큰 배열을 효율적으로 처리할 수 있도록 최적화되어 있음\n",
    "버스에 부하를 줄인다 -> CPU, GPU의 순수 계산 수행 비율이 증가)\n",
    "\n",
    "100개의 이미지 묶음을 한꺼번에 입력 -> 100*10의 2차원 배열 출력 (속도가 더 빠름)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "batch_size = 100\n",
    "accuracy_cnt = 0\n",
    "\n",
    "for i in range(0, len(x), batch_size):\n",
    "    x_batch = x[i:i+batch_size]\n",
    "    y_batch = predict(network, x_batch)\n",
    "    p = np.argmax(y_batch, axis=1)\n",
    "    accuracy_cnt += np.sum(p == t[i:i+batch_size])\n",
    "\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3장 정리\n",
    "\n",
    "신경망 - 활성화 함수로 시그모이드 함수 (매끄럽게 변화하는 함수)\n",
    "퍼셉트론 - 활성화 함수로 계단 함수 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4장 신경망 학습\n",
    "\n",
    "학습 : 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것\n",
    "\n",
    "신경망이 학습할 수 있도록 해주는 지표인 손실 함수를 공부할 예쩡\n",
    "(손실 함수의 결과값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 목표)\n",
    "-> 함수의 기울기를 활용하는 경사법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 주도 학습\n",
    "\n",
    "기계학습의 중심에는 데이터가 존재 -> 주어진 데이터에서 특징을 추출해 그 특징의 패턴을 기계학습 기술로 학습하는 방법이 있다. (이미지의 특징은 보통 벡터로 기술)\n",
    "\n",
    "신경망(딥러닝)은 데이터 그대로를 입력 데이터로 활용해 학습한다. \n",
    "(사람의 개입이 전혀 없이 기계가 스스로 학습)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 데이터와 시험 데이터\n",
    "\n",
    "훈련 데이터만 사용해 최적의 매개변수를 찾는다.\n",
    "시험 데이터를 사용해 훈련한 모델의 실력을 평가\n",
    "\n",
    "나누는 이유? - 범용 능력을 제대로 평가하기 위해서 (아직 보지 못한 데이터를 풀어내는 능력)\n",
    "\n",
    "오버피팅 : 한 데이터셋에만 지나치게 최적화된 상태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실 함수\n",
    "\n",
    "신경망 학습에서는 손실 함수를 기준으로 최적의 매개변수 값을 찾는다.\n",
    "(훈련 데이터를 얼마나 잘 처리하지 못하느냐를 나타냄)\n",
    "\n",
    "일반적으로 오차제곱합과 교차 엔트로피 오차를 사용\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오차제곱합\n",
    "\n",
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "# 교차 엔트로피 오차\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7 # 델타값들 더하는 이유는 0일 경우 -inf가 되는 것을 방지\n",
    "    return -np.num(t * np.log(y +delta))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 미니배치 학습\n",
    "\n",
    "기계학습에서 손실 함수 값을 최대한 줄여주는 매개변수(가중치, 편향)를 찾아야함.\n",
    "따라서 모든 훈련 데이터에 대한 손실 함수 값 구해야함.\n",
    "\n",
    "하지만 모든 데이터에 대한 손실 함수 값을 구해 평균 내는 것은 힘들다.\n",
    "-> 미니배치를 통해 많은 데이터 중 일부만 무작위로 뽑아 학습을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10개만 무작위로 뽑음\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size) # 뽑아내기\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "\n",
    "print(\"x_batch:\", x_batch)\n",
    "print(\"t_batch:\", t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (배치용) 교차 엔트로피 오차 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왜 손실 함수를 설정하는가?\n",
    "-> 높은 정확돌르 끌어내는 매개변수를 찾는 것이 궁극적인 목적\n",
    "이때 매개변수릐 미분을 계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복\n",
    "\n",
    "미분값이 음수면 가중치를 양의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있음\n",
    "미분값이 양수면 가중치를 음의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있음\n",
    "미분값이 0이면 변화시켜도 손실 함수의 값이 줄어들지 않는다. -> 갱신 멈춤\n",
    "\n",
    "\n",
    "*신경망을 학습할 때 정확도를 지표로 삼아서는 안 된다. 정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다.\n",
    "why? 정확도는 33% -> 34% 로 불연속적인 값으로 변화하기 때문에 미소한 변화에는 거의 반응을 보이지 않기 때문이다.\n",
    "이것은 또한 활성화 함수로 계단 함수를 사용하지 않는 이유가 될 수 있음.\n",
    "\n",
    "시그모이드 함수는 미분이 0이 되지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 수치미분\n",
    "\n",
    "경사법에서는 기울기 값을 기준으로 나아갈 방향을 정합니다.\n",
    "\n",
    "미분시 주의사항\n",
    "1. 극한 h->0에서 h로 0에 가까운 값을 넣기 위해 너무 작은 값을 넣으면 반올림 오차로 인해 0이 되어버린다. -> h로 10^-4 정도의 값 사용\n",
    "2. 함수의 차분 -> 중앙 차분 이용하기\n",
    "\n",
    "적용한 수치 미분 구현(아래)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2 수치 미분의 예\n",
    "y = 0.01x^2 + 0.1x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_diff(function_1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_diff(function_1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.3 편미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0 = 3, x1 = 4 편미분 x0\n",
    "\n",
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2.0\n",
    "\n",
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0 = 3, x1 = 4 편미분 x1\n",
    "\n",
    "def function_tmp2(x1):\n",
    "    return 3.0**2 + x1**2.0\n",
    "\n",
    "numerical_diff(function_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄인느 방향 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.1 경사법 (경사 하강법) - 기계학습을 최적화하는데 흔히 쓰는 방법\n",
    "\n",
    "기울기를 잘 이용해 함수의 최솟값을 찾으려는 것이 경사 하강법임\n",
    "\n",
    "기울기가 가리키는 곳에 최솟값이 있는지는 보장할 수 없음.\n",
    "\n",
    "기울기가 가리키는 방향으로 일정거리 이동 후 다시 기울기 측정 \n",
    "-> 반복하여 함수의 값을 점차 줄이는 것이 경사법\n",
    "\n",
    "경사법을 수식으로 나타낸 것에서 에타 기호는 갱신하는 양을 나타내고 신경망 학습에서는 이를 학습률이라고 부른다.\n",
    "\n",
    "이 학습률 값은 미리 특정 값으로 정해두어야 하는데, 너무 크거나 작으면 좋은 장소를 찾아갈 수 없다. 신경망 학습에서는 학습률 값을 변경하면서 올바르게 학습하고 있는지를 확인하면서 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num = 100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**학습률이 너무 크거나 작으면 좋은 결과를 얻을 수 없음**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"/home/bmk/repos/DeepLearning/deep-learning-from-scratch-master\")\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "\n",
    "np.argmax(p)\n",
    "\n",
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda w: net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 학습 알고리즘 구현하기\n",
    "\n",
    "신경망 학습의 순서\n",
    "\n",
    "전제. 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다. 신경망 학습은 다음과 같이 4단계로 수행\n",
    "\n",
    "1단계. 미니배치 - 훈련 데이터 중 일부를 무작위로 가져옴. 선별한 데이터를 미니배치라 부르고 미니배치의 손실 함수 값을 줄이는 것이 목표\n",
    "\n",
    "2단계. 기울기 산출 - 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수 값을 가장 작게 하는 방향을 제시\n",
    "\n",
    "3단계. 매개변수 갱신 - 매개변수를 기울기 방향으로 아주 조금 갱신(이동)\n",
    "\n",
    "4단계. 반복 - 1~3 단계 반복\n",
    "\n",
    "\n",
    "\n",
    "경사 하강법으로 매개변수를 갱신하는 방법이며, 미니배치로 무작위 선정하기 때문에 확률적 경사 하강법(SGD)이라 부름."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2층 신경망 클래스 구현하기\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(\"/home/bmk/repos/DeepLearning/deep-learning-from-scratch-master\")\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        # params : 신경망의 매개변수를 보관하는 딕셔너리 변수\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    # 예측(추론)\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n",
    "\n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    # 손실 함수의 값을 계산\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "\n",
    "        return cross_entropy_error(y, t)\n",
    "\n",
    "    # 정확도 구하기\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        # grads : 신경망 매개변수의 기울기를 보관하는 딕셔너리 변수\n",
    "        # loss_W에는 추론을 하고 정답 레이블과 비교하여 교차 엔트로피 방식으로 구한 손실 함수의 크기를 저장하는 변수이고\n",
    "        # grads 에는 신경망 매개변수를 조금 변화시켰을때 손실 함수가 얼마나 변하는지(기울기)를 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미니배치 학습 구현하기\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    #grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train_acc, test acc : \", str(train_acc) + \", \" + str(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 오차역전파법\n",
    "\n",
    "가중치 매개변수의 기울기를 효율적으로 계산하는 방법임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 계산 그래프\n",
    "\n",
    "그래프 자료구조로 복수의 노드와 에지로 표현됨\n",
    "\n",
    "이점 1. 국소적 계산 - 복잡한 계산을 단순화할 수 있음\n",
    "\n",
    "이점 2. 중간 계싼 결과를 모두 보관 가능\n",
    "\n",
    "이점 3. (가장큰 이점) 역전파를 통해 미분을 효율적으로 계산할 수 있음\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price)\n",
    "\n",
    "# 역전파\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 활성화 함수 계층 구현하기\n",
    "\n",
    "신경망을 구성하는 층(계층) 각각을 클래스 하나로 구현함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu 계층\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그냥 궁금해서 출력해봄\n",
    "import numpy as np\n",
    "x = np.array([[1.0, -1.0], [1.0, -1.0]])\n",
    "\n",
    "mask = (x <= 0)\n",
    "print(mask)\n",
    "out = x.copy()\n",
    "print(out)\n",
    "out[mask] = 0\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid 계층\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Affine / Softmax 계층 구현하기\n",
    "\n",
    "Affine 계층 : 행렬의 곱 계산의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.dot()으로 구현한  Affine 계층\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(2)\n",
    "W = np.random.rand(2, 3)\n",
    "B = np.random.rand(3)\n",
    "\n",
    "X.shape\n",
    "W.shape\n",
    "B.shape\n",
    "\n",
    "Y = np.dot(X, W) + B\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.B\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(Self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3 Softmax with Loss 계층\n",
    "\n",
    "마지막 출력층에서 사용하는 함수이자 정규화를 해준다.\n",
    "정규화 : 출력의 합이 1이 되도록 변형 -> 확률으로 해석 가능\n",
    "\n",
    "softmax함수를 지나기 전 affine함수의 출력을 점수(score)라고 부름\n",
    "\n",
    "소프트맥스 함수와 손실함수(교차 엔트로피 오차)를 합쳐 softmax_with_loss 계층으로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.2 오차역전파법을 적용한 신경망 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"/home/bmk/repos/DeepLearning/deep-learning-from-scratch-master\")\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def loss(self, x ,t):\n",
    "        y = self.predict(x)\n",
    "\n",
    "        return self.lastLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x ,t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        \n",
    "        if t.ndim != 1 :\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.3 오차역전파법으로 구한 기울기 검증하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"/home/bmk/repos/DeepLearning/deep-learning-from-scratch-master\")\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from ch04.two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 차이의 절대값을 구한 후, 그 절댓값들의 평균을 낸다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수치 미분으로 구한 기울기와 오차역전파법으로 구한 기울기의 오차가 매우 작음을 확인함 -> 오차역전파법으로 구한 기울기의 신뢰도 상승"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from ch04.two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)\n",
    "print(train_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5장 정리\n",
    "\n",
    "- 계산 그래프를 이용하면 계산 과정을 시각적으로 파악할 수 있다.\n",
    "- 계산 그래프의 노드는 국소적 계산으로 구성된다. 국소적 계산을 조합해 전체 계산을 구성한다.\n",
    "- 계산 그래프의 순전파는 통상의 계산을 수행한다. 한편, 계산 그래프의 역전파로는 각 노드의 미분을 구할 수 있다.\n",
    "- 신경망의 구성 요소를 계층으로 구현하여 기울기를 효율적으로 계산할 수 있다.(오차역전파법)\n",
    "- 수치 미분과 오차역전파법의 결과를 비교하면 오차역전파법의 구현에 잘못이 없는지 확인할 수 있다.(기울기 확인)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6. 학습 관련 기술들\n",
    "\n",
    "가중치 매개변수의 최적값을 탐색하는 최적화 방법, 가중치 매개변수 초깃값, 하이퍼 파라미터 설정 방법\n",
    "오버피팅의 대응책인 가중치 감소와 드롭아웃 등의 정규화 방법도 간략히 설명하고 구현\n",
    "배치 정규화\n",
    "\n",
    "위 기법들을 이용해 신경망 학습의 효율과 정확도를 높일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 매개변수 갱신\n",
    "\n",
    "매개변수의 기울기(미분)을 이용해 최적값을 찾는 확률적 경사 하강법(SGD)를 사용했음\n",
    "이거보다 똑똑한 방법이 있음. -> SGD의 단점을 알아보고 더 좋은 방법을 사용해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def updatd(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4 모멘텀\n",
    "\n",
    "SGD의 단점 : 기울기가 정확히 최적값을 가리키지 않음\n",
    "-> 모멘텀 기법 나옴\n",
    "\n",
    "W <- W + v\n",
    "\n",
    "v : 속도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.5 AdaGrad\n",
    "\n",
    "신경망 학습에서는 학습률 값이 중요하다. 이 값이 너무 작으면 학습 시간이 길어지고 너무 크면 발산하여 학습이 제대로 이뤄지지 않음.\n",
    "\n",
    "학습률을 정하는 효과적인 기술로 학습률 감소(learning rate decay)가 있음\n",
    "학습률 감소(learning rate decay) : 처음에는 크게 학습하다가 조금씩 작게 학습하기\n",
    "\n",
    "학습률 감소가 매개변수의 원소마다 다르게 적용됨\n",
    "\n",
    "\n",
    "최솟값을 향해 효율적으로 움직이는 것을 알 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7) # 0을 0으로 나누는 경우를 막아줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.6 Adam\n",
    "\n",
    "모멘텀과 AdaGrad를 합친것\n",
    "common/potimizer.py에 구현해두었다고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.7 어느 갱신 방법을 이용할 것인가?\n",
    "\n",
    "1. SGD\n",
    "2. 모멘텀\n",
    "3. AdaGrad\n",
    "4. Adam\n",
    "\n",
    "문제에 따라, 하이퍼파라미터를 어떻게 설정하느냐에 따라 결과가 바뀐다.\n",
    "\n",
    "항상 뛰어난 기법은 아직 없다. 각자 장단점이 있어 문제에 따라 좋은게 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.8 MNIST 데이터셋으로 본 갱신 방법 비교\n",
    "\n",
    " AdaGrad가 가장 빠르고 SGD가 가장 느리다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 가중치의 초깃값\n",
    "\n",
    "신경망 학습에서 특히 중요한 것이 가중치의 초깃값이다. \n",
    "\n",
    "### 6.2.1 초깃값을 0으로 하면?\n",
    "\n",
    "오버피팅을 억제해 범용 성능을 높이는 테크닉 : 가중치 감소 기법\n",
    "\n",
    "가중치 감소는 가중치 매개변수의 값이 작아지도록 학습하는 방법이다.\n",
    "가중치 값을 작게 하여 오버피팅이 일어나지 않게 하는 것\n",
    "\n",
    "하지만 가중치 값을 작게 하기 위해서 초깃값을 0으로 해버리면 학습이 올바로 이뤄지지 않는다. (실제로 우리는 표준편차가 0.01인 정규분포를 사용해 가중치 초기값을 정했음)\n",
    "why? : 오차역전파법에서 모든 가중치의 값이 똑같이 갱신되기 때문!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 은닉층의 활성화값 분포\n",
    "\n",
    "활성화 함수로 시그모이드 함수를 사용하는 5층 신경망에 무작위로 생성한 입려 데이터를 흘리며 각 층의 활성화값 분포를 히스토그램을 그려보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "x = np.random.randn(1000, 100) # 1000개의 데이터\n",
    "node_num = 100 # 각 은닉층의 노드(뉴런) 수\n",
    "hidden_layer_size = 5 # 은닉층이 5개\n",
    "activations = {} # 이곳에 활성화 결과(활성화값)을 저장\n",
    "\n",
    "for i in range(hidden_layer_size):\n",
    "    if i != 0:\n",
    "        x = activations[i-1]\n",
    "\n",
    "    w = np.random.randn(node_num, node_num) / np.sqrt(node_num) # 곱하는 값이 표준 편차로 적용\n",
    "    a = np.dot(x, w)\n",
    "    z = sigmoid(a)\n",
    "    activations[i] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, a in activations.items():\n",
    "    plt.subplot(1, len(activations), i+1)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    plt.hist(a.flatten(), 30, range=(0, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가중치 초깃값을 표준편차 1로 설정\n",
    "\n",
    "각 층의 활성화값들이 0, 1에 치우쳐 분포되어있음 -> 시그모이드 함수에서는 0, 1에 가까워지면 미분값은 0에 다가간다.\n",
    "따라서 데이터가 0, 1에 치우쳐 분표하게 되면 역전파의 기울기 값은 점점 작아지다가 사라진다.\n",
    "이것은 기울기 소실이라 알려진 문제이다. 층을 깊게하는 딥러닝에서 기울기 소실은 심각한 문제가 될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가중치 초깃값을 표준편차 0.01로 설정 \n",
    "\n",
    "0.5 부근으로 활성화값들이 치우친 것을 볼 수 있다.\n",
    "기울기 소실 문제는 일어나지 않지만 뉴런 100개가 거의 같은 값을 출력한다면 뉴런 1개가 출력한 것과 별반 다를 게 없는 것이다. 따라서 표현력을 제한한다는 관점에서 문제가 된다.\n",
    "\n",
    "* 각 층의 활성화 값은 고르게 분포되어야 한다. 적당하게 다양한 데이터가 흘러야 신경망 학습이 효율적으로 이뤄진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가중치 초깃값을 Xavier 초깃값으로 설정\n",
    "\n",
    "각 층의 활성화값들을 광범위하게 분포시킬 목적으로 가중치의 적절한 분포를 찾고자 했고 앞 계층의 노드가 n개라면 표준편차가 1/root(n)인 분포를 사용하면 된다는 결론을 이끌었다.\n",
    "\n",
    "위 초깃값은 앞 층에 노드가 많을수록 대상 노드의 초깃값으로 설정하는 가중치가 좁게 퍼진다.\n",
    "\n",
    "결과로 층이 깊어지면서 형태가 다소 일그러지지만 확실히 넓게 분포되는 것을 확인할 수 있었다. -> 시그모이드 함수의 표현력 제한도 받지 않고 학습이 효율적으로 이뤄질 것으로 기대된다.\n",
    "\n",
    "모양이 일그러지는 것을 tanh 함수로 사용하면 개선된다.\n",
    "sigmoid : 0, 0.5에서 대칭인 S 곡선\n",
    "tanh : 0, 0에서 대칭인 S 곡선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 ReLU를 사용할 때의 가중치 초깃값\n",
    "\n",
    "He 초깃값이다.\n",
    "앞 계층의 노드가 n개일 때, 표준편차가 root(2/n)인 정규분포 사용\n",
    "Relu는 음의 영역이 0이라서 더 넓게 분포시키기 위해 Xavier 초깃값보다 2배의 계수가 필요하다고 해석 가능\n",
    "\n",
    "ReLU에서 He 초깃값을 사용한 결과 모든 층에서 활성화값이 균일하게 분포되는 것을 확인할 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 결론\n",
    "\n",
    "Relu - He 초깃값 사용\n",
    "Sigmoid, tanh 등의 S자 모양 곡선 - Xavier 초깃값 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 배치 정규화\n",
    "\n",
    "각 층이 활성화를 적당히 퍼뜨리도록 강제해버리기 : 배치 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 배치 정규화 알고리즘\n",
    "\n",
    "배치 정규화가 주목받는 이유\n",
    "1. 학습 속도 개선\n",
    "2. 초깃값에 크게 의존 X\n",
    "3. 오버피팅을 억제 (드롭아웃 등의 필요성 감소)\n",
    "\n",
    "미니배치를 단위로 정규화한다.\n",
    "m개의 입력 데이터의 집합에 대해 평균과 분산을 구하고 각각 0, 1이 되도록 적절히 정규화한다.\n",
    "이 처리를 활성화 함수의 앞(또는 뒤)에 삽입함으로써 데이터 분포가 덜 치우치게 만든다.\n",
    "또, 배치 정규화 계층마다 이 정규화된 데이터에 고유한 확대와 이동 변환을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 배치 정규화의 효과\n",
    "\n",
    "가중치 초깃값의 표준편차를 어떻게 하든 거의 모든 경우에서 배치 정규화를 사용했을 때의 학습 진도가 빠르다.\n",
    "추가적으로 초깃값을 제대로 설정하지 않은 경우에도 배치 정규화를 통해 학습이 잘 이뤄지는 것을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 바른 학습을 위해\n",
    "\n",
    "기계학습에서는 오버피팅이 문제가 되는 일이 많다. \n",
    "훈련 데이터에는 포함되지 않는 아직 보지 못한 데이터가 주어져도 바르게 식별해내는 몯레이 바람직하기 때문에 복잡하고 표현력이 높은 모델을 만들 수 있지만 그만큼 오버피팅을 억제하는 기술이 중요해짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.1 오버피팅\n",
    "\n",
    "오버피팅은 주로 두 경우에 일어난다.\n",
    "1. 매개변수가 많고 표현력이 높은 모델\n",
    "2. 훈련 데이터가 적음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2 가중치 감소\n",
    "\n",
    "오버피팅 억제용으로 많이 이용해온 방법 가중치 감소\n",
    "\n",
    "큰 가중치에 대해서는 큰 패널티를 부과하여 오버피팅을 억제하는 방법이다.\n",
    "원래 오버피팅은 가중치 매개변수의 값이 커서 발생하는 경우가 많기 때문\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.3 드롭아웃\n",
    "\n",
    "뉴런을 임의로 삭제하면서 학습하는 방법\n",
    "훈련 때 은닉층의 뉴런을 무작위로 골라 삭제한다.\n",
    "시험 때 모든 뉴런의 신호를 전달.\n",
    "단, 시험 때는 각 뉴런의 출력에 훈련 때 삭제 안 한 비율을 곱하여 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            # mask는 x와 형상이 같은 배열 무작위 생성 후 dropout_ratio 보다 큰 원소만 True로 설정\n",
    "            self.mask = np.random.rand(*x.shpe) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    # 역전파 때의 동작은 ReLU와 동일, 순전파 때 통과시키지 않은 뉴런은 역전파 때도 신호를 차단함\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "드롭아웃을 적용하니 훈련 데이터와 시험 데이터의 정확도 차이가 줄었지만 훈련 데이터에 대한 정확도가 100%에 도달하지 못했다.\n",
    "-> 표현력을 높이면서 오버피팅을 억제할 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 앙상블 학습 (ensemble learning) : 개별적으로 학습시킨 여러 모델의 출력을 평균 내어 추론하는 방식.\n",
    "\n",
    "앙상블 학습은 드롭아웃과 밀접하다.\n",
    "why? 뉴런을 무작위로 삭제하는 행위를 매번 다른 모델을 학습시키는 것으로 해석 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 적절한 하이퍼파라미터 값 찾기\n",
    "\n",
    "- 각 층의 뉴런 수\n",
    "- 배치 크기\n",
    "- 매개변수 갱신 시의 학습률, 가중치 감소 등\n",
    "\n",
    "하이퍼파라미터의 값을 최대한 효율적으로 탐색하는 방법을 알아보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.1 검증 데이터\n",
    "\n",
    "하이퍼파라미터의 성능을 평가할 때는 시험 데이터를 사용하면 안된다.\n",
    "매우 중요하지만 놓치기 쉬운 포인트\n",
    "\n",
    "하이퍼파라미터를 조정용 데이터는 검증 데이터라고 부른다.\n",
    "\n",
    "- 훈련 데이터 : 매개변수 학습\n",
    "- 검증 데이터 : 하이퍼파라미터 성능 평가\n",
    "- 시험 데이터 : 신경망의 범용 성능 평가\n",
    "\n",
    "훈련 데이터 중 20% 정도를 검증 데이터로 먼저 분리하는 간단한 방법이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist()\n",
    "\n",
    "# 훈련 데이터를 뒤섞는다. (순서대로 정렬되어 있을 경우 고려)\n",
    "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
    "\n",
    "validation_rate = 0.2\n",
    "validation_num = int(x_train.shape[0] * validation_rate)\n",
    "\n",
    "x_val = x_train[:validation_num]\n",
    "t_val = t_train[:validation_num]\n",
    "x_train = x_train[validation_num:]\n",
    "t_train = t_train[validation_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.2 하이퍼파라미터 최적화\n",
    "\n",
    "0단계 : 하이퍼파라미터 값의 범위를 설정\n",
    "1단계 : 설정된 범위에서 하이퍼파라미터의 값을 무작위 추출\n",
    "2단계 : 1단계에서 샘플링한 하이퍼파라미터 값을 사용하여 학습하고, 검증 데이터로 정확도를 평가 (단, 에폭은 작게 설정)\n",
    "3단계 : 1, 2단계를 특정 횟수 (100회 등) 반복하며, 그 정확도의 결과를 보고 하이퍼파라미터의 범위를 좁힌다.\n",
    "\n",
    "위 단계들을 반복하여 하이퍼파라미터의 범위를 좁혀하고 어느 정도 좁아지면 압축한 범위에서 값을 하나 골라낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.3 하이퍼파라미터 최적화 구현하기\n",
    "\n",
    "가중치 감소 계수의 범위 : 10^-8 ~ 10^-4\n",
    "학습률의 범위 : 10^-6 ~ 10^-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 10 ** np.random.uniform(-8, -4)\n",
    "lr = 10 ** np.random.uniform(-6, -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 정리\n",
    "\n",
    "- 매개변수 갱신 방법에는 SGD, 모멘텀, AdaGrad, Adam 등이 있다.\n",
    "- 가중치 초깃값을 정하는 방법은 올바른 학습을 하는데 매우 중요하다.\n",
    "- 가중치의 초깃값으로는 Xvaier초깃값과 He초깃값이 효과적이다.\n",
    "- 배치 정규화를 이용하면 학습을 빠르게 진행할 수 있으며, 초깃값에 영향을 덜 받음.\n",
    "- 오버피팅을 억제하는 정규화 기술로는 가중치 감소와 드롭아웃이 있다.\n",
    "- 하이퍼파라미터 값 탐색은 최적 값이 존재할 법한 범위를 점차 좁히면서 하는게 중요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. 합성곱 신경망(CNN)\n",
    "\n",
    "CNN은 이미지 인식과 음석 인식 등 다양한 곳에 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 전체 구조\n",
    "\n",
    "신경망은 인접하는 계층의 모든 뉴런과 결합되어 있었다. 이를 완전 연결(fully-connected, 전결합)이라 한다.\n",
    "완전 연결된 계층을 Affine 계층이라는 이름으로 구현했음.\n",
    "\n",
    "기존 신경망 계층 : Affine - ReLU\n",
    "마지막 출력 계층 : Affine - Softmax\n",
    "\n",
    "CNN에서는 새로운 합성곱 계층과, 풀링 계층이 등장\n",
    "\n",
    "CNN의 계층 : Conv - ReLU - (Pooling) 흐름으로 연결된다.\n",
    "CNN에서는 출력에 가까운 층에서는 기존 신경망 계층을 사용할 수 있고\n",
    "출력 계층은 그대로 마지막 출력 계층을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 합성곱 계층\n",
    "\n",
    "CNN 고유 용어\n",
    "- Padding(패딩)\n",
    "- Stride(스트라이드)\n",
    "\n",
    "각 계층 사이에는 3차원 데이터같이 입체적인 데이터가 흐른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 완전연결 계층의 문제점\n",
    "\n",
    "* 데이터 형상이 무시된다.\n",
    "완전연결 계층에서는 3차원 데ㅣ터를 1차원 데이터로 평탄화해줘야 한다.\n",
    "MNIST 데이터셋을 사용한 사례에서는 28*28 데이터를 784개의 1줄의 데이터로 입력했음\n",
    "\n",
    "따라서 완전연결 계층은 형상에 담긴 정보를 살릴 수 없다.\n",
    "\n",
    "합성곱 계층은 형상을 유지하기 때문에 이미지처럼 형상을 가진 데이터를 이해할 수 있다.\n",
    "\n",
    "합성곱 계층의 입출력 데이터를 특징 맵(feature map)이라고도 한다.\n",
    "input feature map, output feature map\n",
    "입출력 데이터 = 특징 맵 --> 같은 의미\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 합성곱 연산\n",
    "\n",
    "CNN에서는 필터의 매개변수가 그동안의 가중치에 해당\n",
    "편향도 존재함. 편향은 항상 하나만 존재하고 필터를 적용한 모든 원소에 더함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3 패딩\n",
    "\n",
    "입력 데이터 주변을 특정 값(예컨대 0)으로 채우는 것. 합성곱 연산에서 자주 이용\n",
    "\n",
    "목적 : 출력의 크기를 조정하기 위한 것\n",
    "\n",
    "층이 깊어지면 출력의 크기가 작아디자닥 어느 시점에서는 출력의 크기가 1이 되어버린다. 이런 문제를 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.4 스트라이드\n",
    "\n",
    "필터를 적용하는 위치의 간격을 스트라이드라고 한다.\n",
    "예시에서는 스트라이드가 모두 1이었지만 스트라이드가 2가 되면 윈도우가 두 칸씩 이동\n",
    "\n",
    "스트라이드를 키우면 출력 크기는 작아진다.\n",
    "패딩을 키우면 출력 크기는 커진다.\n",
    "\n",
    "출력의 크기를 수식화하여 계산하는 법\n",
    "(기존 크기 + 2*패딩 - 스트라이드) / 필터 크기 = 출력 크기\n",
    "\n",
    "위 값이 정수로 나눠떨어지는 값이어야 한다는 점에 주의를 해야한다. (에외도 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.5 3차원 데이터의 합성곱 연산\n",
    "\n",
    "가로, 세로, 채널?\n",
    "\n",
    "채널쪽으로 특징 맵이 여러개 있다면 채널마다 수행하여 결과를 더해 하나의 출력을 얻음\n",
    "\n",
    "입력 데이터의 채널 수와 필터의 채널 수가 같아야한다는 점에 주의를 해야한다.\n",
    "(+모든 채널의 필터가 같은 크기여야 함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.6 블록으로 생각하기\n",
    "\n",
    "3차원의 합성곱 연산은 데이터와 필터를 직윤면체 블록이라고 생각하면 쉽다.\n",
    "가로 : W\n",
    "세로 : H\n",
    "채널 : C\n",
    "\n",
    "합성곱 연산의 출력으로 다수의 채널을 내보내기 위해서는?\n",
    "-> 필터(가중치)를 여러 개(FN개) 사용하기 -> FN개의 출력 맵 생성\n",
    "\n",
    "편향도 FN개로 브로드캐스트 기능으로 형상이 다른 블록의 덧셈을 쉽게 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.7 배치 처리\n",
    "\n",
    "신경망 처리에서 입력 데이터를 한 덩어리롤 묶어 배치로 처리했음\n",
    "완전연결 신경망을 구현하면서는 이 방식을 지원하여 처리 효율을 높이고, 미니배치 방식의 학습도 지원하도록 했음\n",
    "\n",
    "합성곱 연산도 배치 처리를 지원한다. 각 계층을 흐르는 데이터의 차원을 하나 늘려 4차원 데이터로 저장한다.\n",
    "--> 입력 데이터 : 데이터수 N, 채널 수 C, 높이 H, 너비 W 순으로 저장.\n",
    "(데이터수 N : 배치용 차원)\n",
    "\n",
    "4차원 데이터가 하나 흐르 때마다 데이터 N개에 대한 합성곱 연산이 이루어진다.\n",
    "N회 분의 처리를 한 번에 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 풀링 계층\n",
    "\n",
    "왜 저장이 ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a926afa313b26ae1264fdcf81c726a97e69f6ba2ba780f6aa901948710f8d6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
