{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 파이썬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 파이썬에서는 tab보다 공백 문자 4개를 권장한다. (검색해보니 \"파이썬 코딩 스타일 가이드(PEP 8)에서는 공백 4칸으로 규정하고 있습니다\" 라고 한다)\n",
    "2. class의 메소드들은 첫 번째 인수로 자신(self)를 사용한다.\n",
    "3. 브로드 캐스트 : 형상이 다른 배열끼리 연산\n",
    "4. 파이썬(동적 언어)는 C,C++(정적 언어)보다 처리속도가 느리다. -> 빠른 성능이 요구되는 부분은 C,C++로 해결 (Numpy도 주된 처리는 C,  C++로 구현)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0, 6, 0.1)\n",
    "y1 = np.sin(x)\n",
    "y2 = np.cos(x)\n",
    "\n",
    "plt.plot(x, y1, label =\"sin\")\n",
    "plt.plot(x, y2, linestyle =\"--\", label =\"cos\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"sin&cos\")\n",
    "plt.legend() # 8, 9번 줄의 label을 자동으로 가져온다.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 퍼셉트론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def AND(x1, x2):\n",
    "    w1, w2, theta = 0.5, 0.5, 0.7 # AND 진리표를 만족하는 임의의 값을 넣어줌\n",
    "    tmp = x1*w1 + x2*w2\n",
    "    if tmp <= theta:\n",
    "        return 0\n",
    "    elif tmp > theta:\n",
    "        return 1\n",
    "\n",
    "print(AND(0, 0))\n",
    "print(AND(0, 1))\n",
    "print(AND(1, 0))\n",
    "print(AND(1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "편향을 도입\n",
    "b = -theta 로 해서\n",
    "b + w1x1 + w2x2 가 0보다 작으면 0, 크면 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.5]\n",
      "0.5\n",
      "-0.19999999999999996\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([0, 1])\n",
    "w = np.array([0.5, 0.5])\n",
    "b = -0.7\n",
    "print(w*x)\n",
    "print(np.sum(w*x))\n",
    "print(np.sum(w*x) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def AND(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([0.5, 0.5])\n",
    "    b = -0.7\n",
    "    tmp = np.sum(w*x) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "print(AND(0, 0))\n",
    "print(AND(0, 1))\n",
    "print(AND(1, 0))\n",
    "print(AND(1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치와 편향은 기능이 다르다.\n",
    "\n",
    "가중치는 입력신호가 결과에 주는 영향력(중요도) 조절하는 매개변수\n",
    "편향은 뉴런이 얼마나 쉽게 활성화하느냐를 조정하는 매개변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# NAND 게이트\n",
    "\n",
    "def NAND(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([-0.5, -0.5])\n",
    "    b = 0.7\n",
    "    tmp = np.sum(w*x) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "print(NAND(0, 0))\n",
    "print(NAND(0, 1))\n",
    "print(NAND(1, 0))\n",
    "print(NAND(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# OR 게이트 (AND게이트와 가중치 w, b만 다르다)\n",
    "# b가 두 개의 w보다 작아야한다? - 내 생각\n",
    "\n",
    "def OR(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([0.5, 0.5])\n",
    "    b = -0.2\n",
    "    tmp = np.sum(w*x) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "print(OR(0, 0))\n",
    "print(OR(0, 1))\n",
    "print(OR(1, 0))\n",
    "print(OR(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR게이트는 배타적 논리합이라는 논리 회로.\n",
    "둘 중 하나만 1일때 1을 출력\n",
    "\n",
    "퍼셉트론으로는 비선형 영역을 나타낼 수 없어서 XOR 게이트를 표현 불가\n",
    "-> \"다층 퍼셉트론을 만들 수 있다\"는 점에서 장점이 있다.\n",
    "\n",
    "조합을 통해서 XOR게이트를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def XOR(x1, x2):\n",
    "    s1 = NAND(x1, x2)\n",
    "    s2 = OR(x1, x2)\n",
    "    y = AND(s1, s2)\n",
    "    return y\n",
    "\n",
    "print(XOR(0, 0))\n",
    "print(XOR(0, 1))\n",
    "print(XOR(1, 0))\n",
    "print(XOR(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR게이트는 다층 퍼셉트론이다\n",
    "\n",
    "퍼셉트론은 층을 거듭 쌓으면 비선형적인 표현도 가능하고 컴퓨터가 수행하는 처리 모두 표현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 신경망\n",
    "\n",
    "퍼셉트론의 가중치들을 사람이 수동적으로 해야한다 -> 신경망이 이것을 해결.\n",
    "\n",
    "가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력이 신경망의 중요한 성질이다.\n",
    "\n",
    "신경먕은 입력층 -> 은닉층 -> 출력층 으로 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "활성화 함수 : 활성화하는 역할 ( 0보다 크면 1 아니면 0 출력 - 계단 함수 )\n",
    "ex) y = h(b + w1x1 + w2x2)\n",
    "\n",
    "활성화 함수는 퍼셉트론에서 신경망으로 가기 위한 길잡이\n",
    "\n",
    "*일반적으로 \n",
    "(단순) 퍼셉트론 : 단층 네트워크에서 계단 함수를 활성화 함수로 사용한 모델\n",
    "다층 퍼셉트론 : 신경망 (시그모이드 함수 등의 매끈한 활성화 함수를 사용하는 네트워크)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "활성화 함수 (계단 함수를 다른 함수로 변경하는 것이 신경망으로 나아가는 열쇠)\n",
    "\n",
    "(0). 계단 함수\n",
    "h(x) = 1 if x>0 / 0 else\n",
    "\n",
    "1. 시그모이드 함수\n",
    "h(x) = 1 / 1 + e^-x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계단 함수 구현\n",
    "\n",
    "def step_function(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def step_function(x): # 넘파이 배열까지 지원하는 계단 함수\n",
    "    y = x > 0\n",
    "    return y.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 계단 함수 그래프\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def step_function(x):\n",
    "    return np.array(x > 0, dtype = np.int)\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1) # x 값 설정\n",
    "y = step_function(x) # y값 설정\n",
    "plt.plot(x, y) # 그리기\n",
    "plt.ylim(-0.1, 1.1) # y값 그래프 범위 설정\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26894142, 0.73105858, 0.88079708])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시그모이드 함수 구현\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.array([-1.0, 1.0, 2.0])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_621/2456828036.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# 시그모이드 함수 그래프\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계단 함수와 비교했을때 시그모이드 함수는 곡선을 나타낸다.\n",
    "-> 이 점이 신경망 학습에서 아주 중요한 역할을 한다.\n",
    "\n",
    "공통점 : 비선형 함수이다. (하나의 직선으로 그릴 수 없다)\n",
    "-> 신경망에서는 활성화 함수로 비선형 함수를 사용해야함.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU 함수\n",
    "h(x) = x if x>0 / 0 else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ReLU 함수 그래프\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = relu(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.5, 5.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행렬의 곱 - 1차원:벡터, 2차원:행렬 곱\n",
    "np.dot(A, B) != np.dot(B, A)\n",
    "\n",
    "2x3행렬, 2x2 행렬 곱 연산시 오류 출력 A의 열 수와 B의 행 수를 맞추어야함\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2,)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "# 입력층에서 1층으로 신호 전달\n",
    "X = np.array([1.0, 0.5])\n",
    "W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "B1 = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "print(W1.shape)\n",
    "print(X.shape)\n",
    "print(B1.shape)\n",
    "\n",
    "# 전달된 신호\n",
    "A1 = np.dot(X, W1) + B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.7 1.1]\n",
      "[0.57444252 0.66818777 0.75026011]\n"
     ]
    }
   ],
   "source": [
    "# 1층에서 활성화 함수 사용 후 Z1에 저장\n",
    "Z1 = sigmoid(A1)\n",
    "\n",
    "print(A1)\n",
    "print(Z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(3, 2)\n",
      "(2,)\n",
      "[0.51615984 1.21402696]\n",
      "[0.62624937 0.7710107 ]\n"
     ]
    }
   ],
   "source": [
    "# 1층에서 2층으로 신호 전달\n",
    "\n",
    "W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "B2 = np.array([0.1, 0.2])\n",
    "\n",
    "print(Z1.shape)\n",
    "print(W2.shape)\n",
    "print(B2.shape)\n",
    "\n",
    "# 전달된 신호\n",
    "A2 = np.dot(Z1, W2) + B2\n",
    "\n",
    "# 2층에서 활성화 함수 사용\n",
    "Z2 = sigmoid(A2)\n",
    "\n",
    "print(A2)\n",
    "print(Z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31682708 0.69627909]\n",
      "[0.31682708 0.69627909]\n"
     ]
    }
   ],
   "source": [
    "# 2층에서 출력층으로 신호 전달\n",
    "\n",
    "def identity_function(x): # 항등 함수 (흐름을 통일하기 위해 그냥 구현)\n",
    "    return x\n",
    "\n",
    "W3 = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "B3 = np.array([0.1, 0.2])\n",
    "\n",
    "A3 = np.dot(Z2, W3) + B3\n",
    "Y = identity_function(A3)\n",
    "\n",
    "print(A3)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31682708 0.69627909]\n"
     ]
    }
   ],
   "source": [
    "# 구현 정리\n",
    "\n",
    "def init_network(): # 가중치와 편향 초기화\n",
    "    network = {}\n",
    "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network['B1'] = np.array([0.1, 0.2, 0.3])\n",
    "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network['B2'] = np.array([0.1, 0.2])\n",
    "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "    network['B3'] = np.array([0.1, 0.2])\n",
    "\n",
    "    return network\n",
    "\n",
    "def forward(network, x): # 신호가 순방향으로 전달\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    B1, B2, B3 = network['B1'], network['B2'], network['B3']\n",
    "\n",
    "    a1 = np.dot(x, W1) + B1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + B2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + B3\n",
    "    y = identity_function(a3)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "network = init_network()\n",
    "x = np.array([1.0, 0.5])\n",
    "y = forward(network, x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 소프트맥스 함수\n",
    "\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 출력층의 뉴런 수 정하기\n",
    "\n",
    "분류에서 분류하고 싶은 클래스 수로 설정하는 것이 일반적임\n",
    "ex) 숫자 0~9 분류 -> 출력층 뉴런은 10개\n",
    "\n",
    "## 손글씨 숫자 인식\n",
    "\n",
    "학습된 매개변수 사용, 학습 과정 생략, 추론 과정(순전파)만 구현\n",
    "\n",
    "### MNIST 데이터셋\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(flatten=True, normalize=False)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST의 인수 3 가지 (bool 값)\n",
    "1. normalize : True 픽셀 값을 0~1.0, False 픽셀 값을 0부터 255 사용\n",
    "2. flatten : True 이미지를 1차원 배열로, False 3차원 배열로\n",
    "3. one-hot-encoding : True 0, 1의 배열로 나타냄, False 숫자 형태의 레이블 저장 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망의 추론 처리\n",
    "\n",
    "입력층 뉴런 : 784개 (28*28, 아마 픽셀의 개수?)\n",
    "출력층 뉴런 : 10개 (0~9 숫자 분류)\n",
    "은닉층 뉴런 : 1번-50개 뉴런, 2번-100개 뉴런 (임의의 개수)\n",
    "\n",
    "\n",
    "정규화(normalization) : 데이터를 특정 범위로 변환하는 처리 (전처리의 일종?)\n",
    "전처리(pre-processing) : 신경망의 입력 데이터에 특정 변환을 가하는 것\n",
    "\n",
    "데이터 백색화(whitening) : 데이터를 균일하게 분포시키는 정규화?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = \\\n",
    "        load_mnist(normalize=False, flatten=True, one_hot_label=False)\n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    with open(\"./ch03/sample_weight.pkl\", 'rb') as f: # .이 현재 디렉토리를 뜻함\n",
    "        network = pickle.load(f)\n",
    "\n",
    "    return network\n",
    "\n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmk/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray:0.9207\n"
     ]
    }
   ],
   "source": [
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "accuracy_cnt = 0\n",
    "\n",
    "for i in range(len(x)):\n",
    "    y = predict(network, x[i])\n",
    "    p = np.argmax(y)\n",
    "    if p == t[i]:\n",
    "        accuracy_cnt += 1\n",
    "    \n",
    "print(\"Accuray:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 배치 처리\n",
    "\n",
    "전체적으로 보면 28*28인 2차원 배열 -> 원소가 10개인 1차원 배열 출력\n",
    "\n",
    "배치(batch) : 하나로 묶은 입력 데이터\n",
    "(수치 계산 라이브러리는 대부분 큰 배열을 효율적으로 처리할 수 있도록 최적화되어 있음\n",
    "버스에 부하를 줄인다 -> CPU, GPU의 순수 계산 수행 비율이 증가)\n",
    "\n",
    "100개의 이미지 묶음을 한꺼번에 입력 -> 100*10의 2차원 배열 출력 (속도가 더 빠름)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmk/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "batch_size = 100\n",
    "accuracy_cnt = 0\n",
    "\n",
    "for i in range(0, len(x), batch_size):\n",
    "    x_batch = x[i:i+batch_size]\n",
    "    y_batch = predict(network, x_batch)\n",
    "    p = np.argmax(y_batch, axis=1)\n",
    "    accuracy_cnt += np.sum(p == t[i:i+batch_size])\n",
    "\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3장 정리\n",
    "\n",
    "신경망 - 활성화 함수로 시그모이드 함수 (매끄럽게 변화하는 함수)\n",
    "퍼셉트론 - 활성화 함수로 계단 함수 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4장 신경망 학습\n",
    "\n",
    "학습 : 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것\n",
    "\n",
    "신경망이 학습할 수 있도록 해주는 지표인 손실 함수를 공부할 예쩡\n",
    "(손실 함수의 결과값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 목표)\n",
    "-> 함수의 기울기를 활용하는 경사법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 주도 학습\n",
    "\n",
    "기계학습의 중심에는 데이터가 존재 -> 주어진 데이터에서 특징을 추출해 그 특징의 패턴을 기계학습 기술로 학습하는 방법이 있다. (이미지의 특징은 보통 벡터로 기술)\n",
    "\n",
    "신경망(딥러닝)은 데이터 그대로를 입력 데이터로 활용해 학습한다. \n",
    "(사람의 개입이 전혀 없이 기계가 스스로 학습)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 데이터와 시험 데이터\n",
    "\n",
    "훈련 데이터만 사용해 최적의 매개변수를 찾는다.\n",
    "시험 데이터를 사용해 훈련한 모델의 실력을 평가\n",
    "\n",
    "나누는 이유? - 범용 능력을 제대로 평가하기 위해서 (아직 보지 못한 데이터를 풀어내는 능력)\n",
    "\n",
    "오버피팅 : 한 데이터셋에만 지나치게 최적화된 상태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실 함수\n",
    "\n",
    "신경망 학습에서는 손실 함수를 기준으로 최적의 매개변수 값을 찾는다.\n",
    "(훈련 데이터를 얼마나 잘 처리하지 못하느냐를 나타냄)\n",
    "\n",
    "일반적으로 오차제곱합과 교차 엔트로피 오차를 사용\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오차제곱합\n",
    "\n",
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "# 교차 엔트로피 오차\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7 # 델타값들 더하는 이유는 0일 경우 -inf가 되는 것을 방지\n",
    "    return -np.num(t * np.log(y +delta))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 미니배치 학습\n",
    "\n",
    "기계학습에서 손실 함수 값을 최대한 줄여주는 매개변수(가중치, 편향)를 찾아야함.\n",
    "따라서 모든 훈련 데이터에 대한 손실 함수 값 구해야함.\n",
    "\n",
    "하지만 모든 데이터에 대한 손실 함수 값을 구해 평균 내는 것은 힘들다.\n",
    "-> 미니배치를 통해 많은 데이터 중 일부만 무작위로 뽑아 학습을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "t_batch: [9 9 4 8 2 0 7 8 9 7]\n"
     ]
    }
   ],
   "source": [
    "# 10개만 무작위로 뽑음\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size) # 뽑아내기\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "\n",
    "print(\"x_batch:\", x_batch)\n",
    "print(\"t_batch:\", t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57412, 40037, 41138, 17298, 35274,  5753, 50284, 53865, 54471,\n",
       "       51284])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (배치용) 교차 엔트로피 오차 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왜 손실 함수를 설정하는가?\n",
    "-> 높은 정확돌르 끌어내는 매개변수를 찾는 것이 궁극적인 목적\n",
    "이때 매개변수릐 미분을 계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복\n",
    "\n",
    "미분값이 음수면 가중치를 양의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있음\n",
    "미분값이 양수면 가중치를 음의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있음\n",
    "미분값이 0이면 변화시켜도 손실 함수의 값이 줄어들지 않는다. -> 갱신 멈춤\n",
    "\n",
    "\n",
    "*신경망을 학습할 때 정확도를 지표로 삼아서는 안 된다. 정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다.\n",
    "why? 정확도는 33% -> 34% 로 불연속적인 값으로 변화하기 때문에 미소한 변화에는 거의 반응을 보이지 않기 때문이다.\n",
    "이것은 또한 활성화 함수로 계단 함수를 사용하지 않는 이유가 될 수 있음.\n",
    "\n",
    "시그모이드 함수는 미분이 0이 되지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 수치미분\n",
    "\n",
    "경사법에서는 기울기 값을 기준으로 나아갈 방향을 정합니다.\n",
    "\n",
    "미분시 주의사항\n",
    "1. 극한 h->0에서 h로 0에 가까운 값을 넣기 위해 너무 작은 값을 넣으면 반올림 오차로 인해 0이 되어버린다. -> h로 10^-4 정도의 값 사용\n",
    "2. 함수의 차분 -> 중앙 차분 이용하기\n",
    "\n",
    "적용한 수치 미분 구현(아래)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2 수치 미분의 예\n",
    "y = 0.01x^2 + 0.1x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2999999999986347"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.3 편미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x0 = 3, x1 = 4 편미분 x0\n",
    "\n",
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2.0\n",
    "\n",
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x0 = 3, x1 = 4 편미분 x1\n",
    "\n",
    "def function_tmp2(x1):\n",
    "    return 3.0**2 + x1**2.0\n",
    "\n",
    "numerical_diff(function_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄인느 방향 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.1 경사법 (경사 하강법) - 기계학습을 최적화하는데 흔히 쓰는 방법\n",
    "\n",
    "기울기를 잘 이용해 함수의 최솟값을 찾으려는 것이 경사 하강법임\n",
    "\n",
    "기울기가 가리키는 곳에 최솟값이 있는지는 보장할 수 없음.\n",
    "\n",
    "기울기가 가리키는 방향으로 일정거리 이동 후 다시 기울기 측정 \n",
    "-> 반복하여 함수의 값을 점차 줄이는 것이 경사법\n",
    "\n",
    "경사법을 수식으로 나타낸 것에서 에타 기호는 갱신하는 양을 나타내고 신경망 학습에서는 이를 학습률이라고 부른다.\n",
    "\n",
    "이 학습률 값은 미리 특정 값으로 정해두어야 하는데, 너무 크거나 작으면 좋은 장소를 찾아갈 수 없다. 신경망 학습에서는 학습률 값을 변경하면서 올바르게 학습하고 있는지를 확인하면서 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num = 100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**학습률이 너무 크거나 작으면 좋은 결과를 얻을 수 없음**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"/home/bmk/repos/DeepLearning/deep-learning-from-scratch-master\")\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.20602219  0.18468072 -0.35062362]\n",
      " [ 2.03030589 -0.05020363 -0.23828624]]\n",
      "[ 1.70366199  0.06562516 -0.42483179]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.401094858508889"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "\n",
    "np.argmax(p)\n",
    "\n",
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.45683687  0.08879138 -0.54562825]\n",
      " [ 0.68525531  0.13318707 -0.81844237]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.45683687  0.08879138 -0.54562825]\n",
      " [ 0.68525531  0.13318707 -0.81844237]]\n"
     ]
    }
   ],
   "source": [
    "f = lambda w: net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 학습 알고리즘 구현하기\n",
    "\n",
    "신경망 학습의 순서\n",
    "\n",
    "전제. 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다. 신경망 학습은 다음과 같이 4단계로 수행\n",
    "\n",
    "1단계. 미니배치 - 훈련 데이터 중 일부를 무작위로 가져옴. 선별한 데이터를 미니배치라 부르고 미니배치의 손실 함수 값을 줄이는 것이 목표\n",
    "\n",
    "2단계. 기울기 산출 - 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수 값을 가장 작게 하는 방향을 제시\n",
    "\n",
    "3단계. 매개변수 갱신 - 매개변수를 기울기 방향으로 아주 조금 갱신(이동)\n",
    "\n",
    "4단계. 반복 - 1~3 단계 반복\n",
    "\n",
    "\n",
    "\n",
    "경사 하강법으로 매개변수를 갱신하는 방법이며, 미니배치로 무작위 선정하기 때문에 확률적 경사 하강법(SGD)이라 부름."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2층 신경망 클래스 구현하기\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(\"/home/bmk/repos/DeepLearning/deep-learning-from-scratch-master\")\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        # params : 신경망의 매개변수를 보관하는 딕셔너리 변수\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    # 예측(추론)\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n",
    "\n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    # 손실 함수의 값을 계산\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "\n",
    "        return cross_entropy_error(y, t)\n",
    "\n",
    "    # 정확도 구하기\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        # grads : 신경망 매개변수의 기울기를 보관하는 딕셔너리 변수\n",
    "        # loss_W에는 추론을 하고 정답 레이블과 비교하여 교차 엔트로피 방식으로 구한 손실 함수의 크기를 저장하는 변수이고\n",
    "        # grads 에는 신경망 매개변수를 조금 변화시켰을때 손실 함수가 얼마나 변하는지(기울기)를 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_621/1683797631.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# 기울기 계싼\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m#grad = network.gradient(x_batch, t_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_621/2393388467.py\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/DeepLearning/deep-learning-from-scratch-master/common/gradient.py\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_val\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mfxh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# f(x-h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfxh1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfxh2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_621/2393388467.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mloss_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_621/2393388467.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcross_entropy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/DeepLearning/deep-learning-from-scratch-master/common/functions.py\u001b[0m in \u001b[0;36mcross_entropy_error\u001b[0;34m(y, t)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 미니배치 학습 구현하기\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 기울기 계싼\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    #grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8449b143fe9fdeb0e1f6a99e88c9bd5775019a0f10de0b8469e2a910ef95bc9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
